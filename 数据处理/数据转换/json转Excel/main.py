import os
import csv
import pandas as pd
import ijson

def sanitize_excel_value(value):
    """å»é™¤éæ³•å­—ç¬¦"""
    if isinstance(value, str):
        # å»é™¤ Excel ä¸å…è®¸çš„éæ³•å­—ç¬¦
        value = ''.join([c if ord(c) >= 32 and ord(c) <= 127 else '' for c in value])
    return value

def write_chunk(chunk, fieldnames, output_dir, file_index):
    """
    å°† CSV åˆ‡å—å†™å…¥ Excel æ ¼å¼ï¼Œå¹¶æ¸…ç†éæ³•å­—ç¬¦
    """
    filename = os.path.join(output_dir, f'chunk_{file_index}.csv')
    excel_path = filename.replace('.csv', '.xlsx')

    # å†™å…¥ CSV æ–‡ä»¶
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
        writer.writeheader()
        for item in chunk:
            try:
                writer.writerow(item)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ä¸€æ¡è®°å½•å†™å…¥ CSVï¼š{e}")

    # è¯»å…¥ CSVï¼Œæ¸…ç†éæ³•å­—ç¬¦å¹¶å†™å…¥ Excel
    try:
        df = pd.read_csv(filename, low_memory=False)
        # ä½¿ç”¨ map æ›¿ä»£ applymap æ¥æ¸…ç†å­—ç¬¦
        df = df.apply(lambda col: col.map(sanitize_excel_value) if col.dtype == 'object' else col)
        df.to_excel(excel_path, index=False)
        print(f"ğŸ“„ æˆåŠŸå†™å…¥ï¼š{excel_path}")
    except Exception as e:
        print(f"âŒ å†™å…¥ Excel å¤±è´¥ï¼š{excel_path}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}")

def json_to_csv_chunks(json_path, output_dir, batch_size=10000, progress_step=1000):
    """
    å°† JSON æ–‡ä»¶åˆ†å—å†™å…¥å¤šä¸ª CSV æ–‡ä»¶ï¼Œå¹¶æœ€ç»ˆè½¬æ¢ä¸º Excel æ–‡ä»¶
    """
    fieldnames = set()
    batch = []
    total = 0
    file_index = 0

    # ç¬¬ä¸€æ¬¡æ‰«æï¼Œæ”¶é›†æ‰€æœ‰å­—æ®µå
    print("ğŸ“‹ æ­£åœ¨æ‰«æå­—æ®µ...")
    with open(json_path, 'rb') as f:
        for idx, item in enumerate(ijson.items(f, 'item')):
            if isinstance(item, dict):
                fieldnames.update(item.keys())
            if idx >= 1000:  # æ‰«æå‰1000æ¡å°±å¤Ÿäº†
                break

    fieldnames = list(fieldnames)
    print(f"âœ… æ£€æµ‹åˆ°å­—æ®µæ•°ï¼š{len(fieldnames)}")

    # ç¬¬äºŒæ¬¡æ­£å¼å†™å…¥ CSV
    print("ğŸ“‚ æ­£åœ¨å¤„ç†æ•°æ®å¹¶å†™å…¥ CSV æ–‡ä»¶...")
    with open(json_path, 'rb') as f:
        for idx, item in enumerate(ijson.items(f, 'item')):
            batch.append(item)
            total += 1

            if total % batch_size == 0:
                print(f"âœ… å·²å¤„ç† {total} æ¡è®°å½•...")
                write_chunk(batch, fieldnames, output_dir, file_index)
                file_index += 1
                batch.clear()

            if total % progress_step == 0:
                print(f"âš™ï¸ è¿›åº¦ï¼šå·²å¤„ç† {total} æ¡è®°å½•...")

        # å¤„ç†å‰©ä½™æ•°æ®
        if batch:
            write_chunk(batch, fieldnames, output_dir, file_index)

    print(f"ğŸ‰ æ€»å…±æˆåŠŸå†™å…¥ {total} æ¡è®°å½•åˆ° CSV å¹¶åˆ†å—ä¸ºå¤šä¸ªæ–‡ä»¶ï¼š{output_dir}")

# ç¤ºä¾‹ç”¨æ³•
json_file = "tickets1.json"        # æ›¿æ¢ä¸ºä½ çš„ JSON è·¯å¾„
output_dir = "output"              # è¾“å‡ºç›®å½•

os.makedirs(output_dir, exist_ok=True)
json_to_csv_chunks(json_file, output_dir, batch_size=50000, progress_step=10000)
